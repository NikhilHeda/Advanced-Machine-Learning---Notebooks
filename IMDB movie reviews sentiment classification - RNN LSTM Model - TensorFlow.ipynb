{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 2), (25000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_one_hot = to_categorical(y_test)\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot.shape, y_train_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing sestences\n",
    "max_review_len = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "NUM_CLASS = 2\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "EMBEDDING_SIZE = 32\n",
    "\n",
    "RNN_SIZE = 50\n",
    "NUM_CHUNK = 10\n",
    "CHUNK_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_placeholder = tf.placeholder(tf.int32, [None, max_review_len])\n",
    "sentiment_placeholder = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    weights = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(weights)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Embedding'):\n",
    "    embedding_weights = weight_variable([VOCAB_SIZE, EMBEDDING_SIZE])\n",
    "    embedding_bias = bias_variable([EMBEDDING_SIZE])\n",
    "    \n",
    "    embedding = tf.nn.embedding_lookup(embedding_weights, text_placeholder) + embedding_bias\n",
    "\n",
    "with tf.name_scope('RNN'):\n",
    "    lstm_weights = weight_variable([RNN_SIZE, NUM_CLASS])\n",
    "    lstm_bias = bias_variable([NUM_CLASS])\n",
    "    \n",
    "    lstm = rnn.BasicLSTMCell(RNN_SIZE)\n",
    "    \n",
    "    #embedding_transpose = tf.transpose(embedding, [1, 0, 2])\n",
    "    #embedding_reshaped = tf.reshape(embedding_transpose, [-1, CHUNK_SIZE])\n",
    "    #embedding_split = tf.split(embedding_reshaped, NUM_CHUNK, 0)\n",
    "    \n",
    "    #outputs, states = rnn.static_rnn(lstm, embedding_split, dtype=tf.float32)\n",
    "    \n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm, embedding, dtype=tf.float32)\n",
    "    \n",
    "    value = tf.transpose(outputs, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    \n",
    "    pred = tf.matmul(last, lstm_weights) + lstm_bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I348533\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 391/391 [02:39<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8695200085639954%\n",
      "Validation Accuracy: 0.852840006351471%\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 391/391 [02:38<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.896399974822998%\n",
      "Validation Accuracy: 0.8634399771690369%\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 391/391 [02:42<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9132400155067444%\n",
      "Validation Accuracy: 0.8684800267219543%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_op = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(labels=sentiment_placeholder, logits=pred))\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss_op)\n",
    "\n",
    "# Evaluations\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(sentiment_placeholder, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # Summaries\n",
    "    LOG_DIR = \"tmp/log/\"\n",
    "    summary_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
    "    summary_writer.close()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print('Epoch {}'.format(epoch + 1))\n",
    "\n",
    "        for index, offset in tqdm(list(enumerate(range(0, X_train.shape[0], BATCH_SIZE))), ncols=100):\n",
    "            xs, ys = X_train[offset: offset + BATCH_SIZE], y_train_one_hot[offset: offset + BATCH_SIZE]\n",
    "            sess.run(train_op, feed_dict={\n",
    "                text_placeholder: xs,\n",
    "                sentiment_placeholder: ys\n",
    "            })\n",
    "\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            text_placeholder: X_train,\n",
    "            sentiment_placeholder: y_train_one_hot\n",
    "        })\n",
    "        validation_accuracy = accuracy.eval(feed_dict={\n",
    "            text_placeholder: X_test,\n",
    "            sentiment_placeholder: y_test_one_hot\n",
    "        })\n",
    "        print('Training Accuracy: {}%\\nValidation Accuracy: {}%\\n'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
